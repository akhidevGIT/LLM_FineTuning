{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sKPTsSASB_Gm"
   },
   "source": [
    "## Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y8n1aiZVLt93",
    "outputId": "60871f55-b9dc-434b-84f7-83a1be30ab87"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.1/60.1 MB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "# Install all necessary libraries\n",
    "!pip install -q transformers datasets peft accelerate bitsandbytes evaluate sentencepiece --prefer-binary\n",
    "\n",
    "# Clear GPU memory if Colab reused a session\n",
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8VkczbbICL-V"
   },
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "XalcFm3Sy4tR"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    Trainer, TrainingArguments, DataCollatorForLanguageModeling, BitsAndBytesConfig\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, TaskType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OVk2c4TfCRLW"
   },
   "source": [
    "## Load Base Model & Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 692,
     "referenced_widgets": [
      "d0c70f8333bd4e459b4b90e40ca8750e",
      "e2f99f83ebdf455a91f380afb904606e",
      "5d5e06648be24aab8a6c2a420009a4e8",
      "7f9658d0ba9e4cc3a5722df69f29e3af",
      "a251854b5a874716ab14326ac8b18ae6",
      "0569e78633694dbabb6854563cf339a4",
      "a50488b03a724347afe4df259616989d",
      "374ca4b48e584eedbe3181f1b139c5a4",
      "e6559cc7e3df4fce861bf58b148cc7ac",
      "d7133b23df7e4cf3be82562ac9a0a247",
      "6f208f9b42eb49adaaa4bce61477d75d",
      "fd3a5d6dd42e4bb5b78d5a819cb37d7f",
      "9d487297aec84ea3a7045dfad9887c30",
      "bb65ead867294542a92023ac0cf45d91",
      "8e6a6872d08b4a60a244c294088b8def",
      "d14995590ea94e05b4a6d35fd4a3577e",
      "6b0ea98e46e9465292254e39e86f1137",
      "013671760ee146bd930a7dff750cfc3d",
      "a3960897345d4ecc98c5cd84699de3ab",
      "30ec15c36f2d488fa122cf3c36289713",
      "cbc25082398c4ad9951df37f5294fb87",
      "0b501842e2fc42659676f485978e6d54",
      "a0edce30dccf454587d3956479796ab6",
      "4bed311ac82a483885fa511372c8ed22",
      "d03f1cb7a0f04575aa9f6c9aa0e20809",
      "19295968ec7f476b95c18b6758ba0dd3",
      "a32f7fb1de70425d8774d5cc46d15179",
      "721301812a454636bf9bb580c86a03e1",
      "93979cc42aa543fc85944080fdad3d1f",
      "e486651d22474a2b9af5262f4b9f8507",
      "051561a7e7ff41eca3d73da5d84f3bd8",
      "ac79fb89629d4e249f0b0a6ea0517ed6",
      "871fee0b9033485ea37fdafcee2de8b6",
      "3264a6ccc59d43fa8e619840f7c95a89",
      "38ad2685495c43c2abbcf24f8b190015",
      "7b875c6bc137450f956b78cc545711d0",
      "60765de26fc24842addaafcaf6338581",
      "35e8441ada61464db68a6ebcfec88999",
      "edcf0286ec014215a83a85bf6a3e0646",
      "07e32a1702b040eeabd4361a0cc9a68a",
      "1b17977160cc4ce18949dc940f1708f9",
      "c151fdd166e34bfea0fcd0f016005eb5",
      "a1ca7c1a2fdb458c911ceb19ced25391",
      "04a50f202b834926af39b1c1b097df83",
      "3e95dab2e91b47f69eadd7d78dc89ad1",
      "18c8d0d83a7d4aea82f4b26dc18a38f1",
      "f8b7d588d8e640e19256ee79e4aa75da",
      "7490da1a1eb840a297bddf0c38cc8e8f",
      "d8fd118a210d4fc8b2900cc5ea2cb3ec",
      "9da77d50c10a4984a4dfe4ca9da399fc",
      "cdbba212fa7c4041b0908e103e960f67",
      "bf784aa18eda40c18659d76111ab4b4f",
      "7eb6c1d545b847b18151f87cb6363083",
      "7d468bc2d47745a4996059e3c03da15f",
      "03d712fbc2244988bb838837c9c4b017",
      "76a8ae863dd04bcaa8f488ad461ae999",
      "c5d4570c036440b2bed53054a9f4bbd9",
      "55039b7dbff84e268997b195035f5d27",
      "d9a4321177e743ada4b09a0c309d3f74",
      "24f5987cb2c54445bac5f24c26de9526",
      "fb03ac16b8fa49b0afa5e349f4df8cd5",
      "67a38272298f4d14be40155902e6ab84",
      "4bcccf795d7747b0815da9723330a6ce",
      "b788620104fd4320b12cb21475332fa4",
      "b7cf27d440504cfb8864dd2a820dd88d",
      "59ffd8d99a7546c0bbf7cf53a3c2aca4",
      "b12767a44edc462ab86a8c2225b2c3a4",
      "baa9c72acb604aa88b6af48d41c0bb2f",
      "c8aaa292ad454f7c92bce27af411b6c8",
      "a9d2f10a98004b2087ec98d18507c32a",
      "22b13e421e87460ea231c659a3baf1d3",
      "392d37fc9dab4d82808af715f0d8a13e",
      "bece56bb2cd749daa25dfcf9297fff39",
      "0bd044d465b24e8099587596c39381d7",
      "4e6bcf7bf0bf4ae7b976f6f0b97444d0",
      "c510332cec434589981fe64b750ecb39",
      "ece761a8108d4483b6798b19500a5267"
     ]
    },
    "id": "cte__-2fz6Vo",
    "outputId": "91d75358-d665-4297-8f8d-3112e314d54c"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0c70f8333bd4e459b4b90e40ca8750e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd3a5d6dd42e4bb5b78d5a819cb37d7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0edce30dccf454587d3956479796ab6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3264a6ccc59d43fa8e619840f7c95a89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/551 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e95dab2e91b47f69eadd7d78dc89ad1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/608 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76a8ae863dd04bcaa8f488ad461ae999",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.20G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b12767a44edc462ab86a8c2225b2c3a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(32000, 2048)\n",
      "    (layers): ModuleList(\n",
      "      (0-21): 22 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear8bitLt(in_features=2048, out_features=2048, bias=False)\n",
      "          (k_proj): Linear8bitLt(in_features=2048, out_features=256, bias=False)\n",
      "          (v_proj): Linear8bitLt(in_features=2048, out_features=256, bias=False)\n",
      "          (o_proj): Linear8bitLt(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear8bitLt(in_features=2048, out_features=5632, bias=False)\n",
      "          (up_proj): Linear8bitLt(in_features=2048, out_features=5632, bias=False)\n",
      "          (down_proj): Linear8bitLt(in_features=5632, out_features=2048, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2048, out_features=32000, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# TinyLlama is a small open model (Llama2 style)\n",
    "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "\n",
    "# Load model in 8-bit precision to save VRAM\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config = BitsAndBytesConfig(load_in_8bit=True),   # quantized weights\n",
    "    device_map=\"auto\"      # automatically uses GPU\n",
    ")\n",
    "\n",
    "# Ensure pad token exists (TinyLlama uses EOS as padding)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "70pujg2rGMmP"
   },
   "source": [
    "## Load and Explore Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 353,
     "referenced_widgets": [
      "347793f6f5fa43b689781279e52d31ae",
      "b393bb59657747bf935b6c1efd79444d",
      "fe47a24f7e3646c28b1feaf389177a03",
      "16969f9f24db4a30a80a210757e89953",
      "65ab4a13ed80421fa9e66a2b7e82c801",
      "cdeda19fd2ae408eb1392169443cd52a",
      "f70ee3ff65fb48dc9743e43a516895a3",
      "bb34b74f260d4838859f147145265a89",
      "68b7949816eb42bc97612d27726a0d67",
      "ac327fbcb6994e05bc9844a10f3da8a1",
      "d91e4f43c284441b8ea5d45e1f549a5a",
      "86b9629904cd4b548c017c7c787b64d3",
      "cd6684091997453e9f2fe6d7d9b80e0a",
      "c81e23d0cf6a4dbcb35b92cce3a8eb34",
      "fbb703070ba94ef28a1d0a3fe15e0f15",
      "d286470cd8cb4cbfbefdf0799a05413b",
      "cc933892b40640f29ee099f938b7b530",
      "62982225dd1340a6b51b8a3faedd3927",
      "05806a94738e43589312587b0ee1e60f",
      "80a4d58e280b43bba12d1895b4e39905",
      "9aa67d6e4ae44100908e20167c76b1b4",
      "b57a0a4205a34f9abf895884777fecb9",
      "652b28618031423a88b4d357f7e5793a",
      "aebe2ac3a76343b4ad45a729456ac9f2",
      "418e0ec23e844f2790c58b469374da39",
      "d54dfc0439d04a61af934f483dc95b9f",
      "62f025bfe171441b9090f234a39805e4",
      "e9c5da8d4b304d0498a8d7fd9eaffc7c",
      "4a2dbc8d7d404bec885325fc6cacf736",
      "4ba1f6ccf28b4373950b8eb88338b971",
      "f45dd5237d2c47c9a59d44c2b6c72640",
      "44b8643a20b5447e9f5abe949c4c8078",
      "df4321911f4c4f21a250f42e80958ddb",
      "6e7b117341e34d4ca69c22dbad788f1a",
      "dc2890092e28487986f7cd3d97979c53",
      "4a127a01130c4eb080dfe70e72c76bc6",
      "dfa342e4786247469529178659efcce5",
      "5d95d8309b0b4af48d388106c278ad0c",
      "98596af058fd43d68445326ac7f0c541",
      "d43b15ad8a0c49eebf041af173571b9b",
      "5ca8888e2fea4ccd980dd879c14f8237",
      "0a308bc9e1c04193a352d13b3d02ce58",
      "991760ea539c467e93a98a37438da246",
      "04f15eab03cc4864b2563e70db27a511"
     ]
    },
    "id": "sG0zNhuJEJuG",
    "outputId": "ff18be05-a8b6-422e-fe65-97681ab6eb6f"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "347793f6f5fa43b689781279e52d31ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86b9629904cd4b548c017c7c787b64d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pqa_labeled/train-00000-of-00001.parquet:   0%|          | 0.00/1.08M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "652b28618031423a88b4d357f7e5793a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['pubid', 'question', 'context', 'long_answer', 'final_decision'],\n",
      "        num_rows: 1000\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e7b117341e34d4ca69c22dbad788f1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['pubid', 'question', 'context', 'long_answer', 'final_decision', 'prompt', 'answer'],\n",
      "        num_rows: 1000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Load PubMed QA (medical question answering dataset)\n",
    "dataset = load_dataset(\"qiaojin/PubMedQA\", \"pqa_labeled\")\n",
    "print(dataset)\n",
    "\n",
    "# Each record has: question, context, long_answer, final_decision\n",
    "# Let's format it for LLM input/output\n",
    "def format_example(example):\n",
    "  prompt = f\"Question: {example['question']}\\nContext: {\".\".join(example['context']['contexts'])}\\nAnswer:\"\n",
    "  answer = example['long_answer']\n",
    "  return {\"prompt\": prompt, \"answer\": answer}\n",
    "\n",
    "dataset = dataset.map(format_example)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Muhry7o4GZst"
   },
   "source": [
    "## Prepare Train/Test Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "jOuoMYAjE5W-"
   },
   "outputs": [],
   "source": [
    "# Select smaller subset for demo\n",
    "train_dataset = dataset[\"train\"].select(range(0, 800))\n",
    "test_dataset  = dataset[\"train\"].select(range(800, 1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NLCZOssyG0Jc"
   },
   "source": [
    "## Tokenize Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81,
     "referenced_widgets": [
      "00a46077ed9945bd9b02132542500055",
      "052fee4b58c5485b87e870ab34b53abd",
      "23a2972f121d403d927c176b8193c0dd",
      "3fc29e9339e34da782926f9eb6b6c5b1",
      "9602b13cb0964989a508cfd81f5673b1",
      "b10c77331d1544bdb71b33837434ae2d",
      "be0dcb9178b44858a8ffd558a7493dd3",
      "fff20e95e1b744269d02268b61101c53",
      "499dd0288a4442b2b42840479a29a287",
      "ceb711600e7b43ffaac3142ce42e9d7e",
      "654055e63c194df086332ff197430746",
      "54bb7bc42948499787bddc5e902332fe",
      "db4821e957da45cf9034930d63d0f840",
      "3b5cee1194ba46c9bb608a6201d71d8a",
      "b69a4a76c4ba4d1cb007b2870665bfb3",
      "d2bfd4eebbaf41779eff8ae8bd7d7b3e",
      "1f5da5f10dd74741bc616241901ec60f",
      "5b00c1b4ccea45bdbb7eeaed26a666eb",
      "75a70a40da3c4b69a2b9192d412f7e4e",
      "2e77c56f1ac746eb931ffd8f7537bb0f",
      "0151346ff531468e9d7163d1d448bf14",
      "15cf3250db11437bbe403be180029f62"
     ]
    },
    "id": "1s5GBwOWOV9S",
    "outputId": "e3ab6154-32cb-4616-fae0-04fc06bf7f61"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00a46077ed9945bd9b02132542500055",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54bb7bc42948499787bddc5e902332fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Function to tokenize prompt+answer pairs\n",
    "def tokenize_function(example):\n",
    "  full_text = [\n",
    "        \" \".join(p) + \" \" + \" \".join(a)\n",
    "        for p, a in zip(example[\"prompt\"], example[\"answer\"])\n",
    "    ]\n",
    "  tokenized = tokenizer(\n",
    "        full_text,               # Creates one full_text per example\n",
    "        truncation=True,         # cut long text\n",
    "        padding=\"max_length\",    # pad shorter text\n",
    "        max_length=256           # 256 tokens max\n",
    "  )\n",
    "  # For causal language modeling, labels = input_ids\n",
    "  tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
    "  return tokenized\n",
    "\n",
    "tokenized_train = train_dataset.map(tokenize_function, batched=True, remove_columns=train_dataset.column_names)\n",
    "tokenized_test  = test_dataset.map(tokenize_function, batched=True, remove_columns=test_dataset.column_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3tcM78c4HDwF"
   },
   "source": [
    "## Setup LoRA (Low-Rank Adaptation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n_4gl2QDjubN",
    "outputId": "1507d2e6-8b68-4607-8f92-22360582310c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,126,400 || all params: 1,101,174,784 || trainable%: 0.1023\n"
     ]
    }
   ],
   "source": [
    "# LoRA adds small trainable matrices instead of retraining full model\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=8,                        # rank dimension\n",
    "    lora_alpha=32,              # scaling factor\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # apply LoRA on attention projections\n",
    "    lora_dropout=0.05,          # small dropout for regularization\n",
    "    bias=\"none\",                # don't fine-tune bias terms\n",
    "    task_type=TaskType.CAUSAL_LM\n",
    ")\n",
    "from peft import prepare_model_for_kbit_training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()  # Shows how many params will train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C6p1s49IICv6"
   },
   "source": [
    "## Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "T36KlVDFqAyR"
   },
   "outputs": [],
   "source": [
    "# Data collator batches variable-length sequences efficiently\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    "\n",
    "# Define training configuration\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./tinyllama-medical-lora\",   # where to save model checkpoints\n",
    "\n",
    "    # Training\n",
    "    num_train_epochs=2,                      # 1–3 is typical for LoRA; 2 is a good start\n",
    "    per_device_train_batch_size=2,           # small batch size = lower VRAM usage\n",
    "    gradient_accumulation_steps=4,           # effectively makes batch size = 2 * 4 = 8\n",
    "    learning_rate=2e-4,                      # recommended LoRA LR\n",
    "    max_grad_norm=0.3,                       # gradient clipping — prevents exploding gradients\n",
    "    weight_decay=0.01,                       # regularization for stability (slightly higher than 0.001)\n",
    "\n",
    "    # Evaluation\n",
    "    eval_strategy=\"steps\",             # correct parameter name (not eval_strategy)\n",
    "    eval_steps=100,                          # evaluate every 100 steps\n",
    "    per_device_eval_batch_size=2,            # batch size for evaluation\n",
    "    save_strategy=\"steps\",                   # save checkpoints every N steps\n",
    "    save_steps=200,                          # save model every 200 steps\n",
    "    load_best_model_at_end=True,             # automatically reload best model\n",
    "\n",
    "    # Optimization\n",
    "    optim=\"paged_adamw_32bit\",               # optimized AdamW for low memory\n",
    "    lr_scheduler_type=\"cosine\",              # cosine decay learning rate\n",
    "    gradient_checkpointing=True,             # saves memory by not storing intermediate activations\n",
    "\n",
    "    # Precision & Performance\n",
    "    fp16=True,                               # use float16 if Colab GPU supports it (T4, A100, etc.)\n",
    "    bf16=False,                              # only use bf16 if GPU supports it (A100+)\n",
    "    group_by_length=True,                    # batches sequences of similar lengths for efficiency\n",
    "\n",
    "    # Logging\n",
    "    logging_steps=25,                        # log metrics every 25 steps\n",
    "    report_to=\"tensorboard\",                 # use TensorBoard for tracking (disable wandb)\n",
    "\n",
    "    # Misc\n",
    "    max_steps=-1,                            # -1 = ignore, use full dataset for num_train_epochs\n",
    ")\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test,\n",
    "    data_collator=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TJO5PTqmH9nB"
   },
   "source": [
    "## Fine-Tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 227
    },
    "id": "YgZiv5F4xa03",
    "outputId": "9111742d-d229-41c1-fa19-5d402ebb5262"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/bitsandbytes/autograd/_functions.py:181: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [200/200 09:42, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.900600</td>\n",
       "      <td>0.893563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.843900</td>\n",
       "      <td>0.884650</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=200, training_loss=0.8684183216094971, metrics={'train_runtime': 585.4313, 'train_samples_per_second': 2.733, 'train_steps_per_second': 0.342, 'total_flos': 2545185875558400.0, 'train_loss': 0.8684183216094971, 'epoch': 2.0})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start LoRA fine-tuning\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "URTW4ywuQX_l"
   },
   "source": [
    "## Evaluate / Test Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Cfbu85mxQLkR",
    "outputId": "28742c49-497f-4962-a777-a1614ab2252f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Can nurse-led preoperative education reduce anxiety and postoperative complications of patients undergoing cardiac surgery?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/bitsandbytes/autograd/_functions.py:181: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: Question: Can nurse-led preoperative education reduce anxiety and postoperative complications of patients undergoing cardiac surgery?\n",
      "Context: {'contexts': ['The effect of preoperative education on anxiety and postoperative outcomes of cardiac surgery patients remains unclear.AIM: The aim of the study was to estimate the effectiveness of a nurse-led preoperative education on anxiety and postoperative outcomes.', 'A randomised controlled study was designed. All the patients who were admitted for elective cardiac surgery in a general hospital in Athens with knowledge of the Greek language were eligible to take part in the study. Patients in the intervention group received preoperative education by specially trained nurses. The control group received the standard information by the ward personnel. Measurements of anxiety were conducted on admission-A, before surgery-B and before discharge-C by the state-trait anxiety inventory.', 'The sample consisted of 395 patients (intervention group: 205, control group: 190). The state anxiety on the day before surgery decreased only in the intervention group (34.0 (8.4) versus 36.9 (10.7); P=0.001). The mean decrease in state score during the follow-up period was greater in the intervention group (P=0.001). No significant difference was found in the length of stay or readmission. Lower proportions of chest infection were found in the intervention group (10 (5.3) versus 1 (0.5); P=0.004). Multivariate linear regression revealed that education and score in trait anxiety scale on admission are independent predictors of a reduction in state anxiety.'], 'labels': ['BACKGROUND', 'METHODS', 'RESULTS'], 'meshes': ['Adult', 'Aged', 'Aged, 80 and over', 'Anxiety Disorders', 'Cardiac Surgical Procedures', 'Female', 'Humans', 'Male', 'Middle Aged', 'Nurse-Patient Relations', 'Patient Education as Topic', 'Postoperative Complications', 'Preoperative Care', 'Preoperative Period'], 'reasoning_required_pred': ['y', 'e', 's'], 'reasoning_free_pred': ['y', 'e', 's']}\n",
      "Answer: The study found that nurse-led preoperative education reduced anxiety and postoperative complications in cardiac surgery patients. The study suggests that nurse-led preoperative education may be an effective way to reduce anxiety and improve postoperative outcomes in patients undergoing cardiac surgery. However, further research is needed to confirm the effectiveness of nurse-led preoperative education in reducing anxiety and postoperative complications in different populations and settings.\n"
     ]
    }
   ],
   "source": [
    "def generate_answer(question, context):\n",
    "    prompt = f\"Question: {question}\\nContext: {context}\\nAnswer:\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    outputs = model.generate(**inputs)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "sample = test_dataset[2]\n",
    "print(\"Question:\", sample[\"question\"])\n",
    "print(\"Answer:\", generate_answer(sample[\"question\"], sample[\"context\"]))\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
